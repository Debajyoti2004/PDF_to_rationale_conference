{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_texts(texts, max_length=512):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='tf'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "csv_file_path = r\"C:\\Users\\Debajyoti\\OneDrive\\Desktop\\project task-1\\data\\updated_publishable_data.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "texts = df['PDF'].tolist() \n",
    "labels = df['Label'].tolist()\n",
    "\n",
    "labels_tensor = tf.convert_to_tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenize_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_ids, attention_masks = tokenized_data['input_ids'], tokenized_data['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 512)\n",
      "(15, 512)\n"
     ]
    }
   ],
   "source": [
    "X_train = (input_ids, attention_masks)\n",
    "Y_train = labels_tensor\n",
    "print(input_ids.shape)\n",
    "print(attention_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(text, tokenizer, max_len=512, stride=256):\n",
    "    \n",
    "    tokens = tokenizer.encode(text, truncation=False, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), stride):\n",
    "        chunk = tokens[i:i + max_len]\n",
    "        chunk = tokenizer.build_inputs_with_special_tokens(chunk)  \n",
    "        chunks.append(chunk)\n",
    "        if len(chunk) < max_len:\n",
    "            break\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_ids           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_mask      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bert_layer          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BertLayer</span>)         │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">769</span> │ bert_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_ids           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_mask      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bert_layer          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mBertLayer\u001b[0m)         │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m769\u001b[0m │ bert_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">769</span> (3.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m769\u001b[0m (3.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">769</span> (3.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m769\u001b[0m (3.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TFBertModel, BertTokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, bert_model_name='bert-base-uncased', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.bert = TFBertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask = inputs\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.pooler_output  # Use pooled output for classification tasks\n",
    "\n",
    "input_ids_layer = tf.keras.Input(shape=(512,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask_layer = tf.keras.Input(shape=(512,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "bert_output = BertLayer()(inputs=[input_ids_layer, attention_mask_layer])\n",
    "\n",
    "output = layers.Dense(1, activation='sigmoid')(bert_output)\n",
    "\n",
    "model = Model(inputs=[input_ids_layer, attention_mask_layer], outputs=output)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler\n",
    "import tensorflow as tf\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def __init__(self, log):\n",
    "        super().__init__()\n",
    "        self.log = log\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        accuracy = logs.get(\"accuracy\")\n",
    "        self.log[epoch] = logs  \n",
    "        if accuracy is not None and accuracy >= 0.90:\n",
    "            print(f\"Accuracy reached {accuracy:.2f} at epoch {epoch + 1}.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch > 0 and epoch % 10 == 0:\n",
    "        return lr * 0.1\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 713ms/step - accuracy: 0.6083 - loss: 0.7295 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 2/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 685ms/step - accuracy: 0.7867 - loss: 0.5458 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 3/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 683ms/step - accuracy: 0.7950 - loss: 0.5212 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 4/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 690ms/step - accuracy: 0.7700 - loss: 0.4671 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 5/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 672ms/step - accuracy: 0.8450 - loss: 0.4017 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 6/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 676ms/step - accuracy: 0.6450 - loss: 0.5510 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 7/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 788ms/step - accuracy: 0.7700 - loss: 0.3945 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 8/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 810ms/step - accuracy: 0.7700 - loss: 0.3959 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 9/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 812ms/step - accuracy: 0.8117 - loss: 0.3498 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 10/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 724ms/step - accuracy: 0.7200 - loss: 0.4020 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 11/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 784ms/step - accuracy: 0.8383 - loss: 0.3386 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 12/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 700ms/step - accuracy: 0.8383 - loss: 0.3437 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 13/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 701ms/step - accuracy: 0.8883 - loss: 0.3187 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 14/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 688ms/step - accuracy: 0.7633 - loss: 0.3917 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 15/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 688ms/step - accuracy: 0.9050 - loss: 0.3067 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 16/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 682ms/step - accuracy: 0.9050 - loss: 0.2864 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 17/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 700ms/step - accuracy: 0.8633 - loss: 0.3078 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 18/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 719ms/step - accuracy: 0.8633 - loss: 0.3095 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 19/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 702ms/step - accuracy: 0.9300 - loss: 0.2803 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 20/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 703ms/step - accuracy: 0.9300 - loss: 0.2829 - learning_rate: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "log = {}  # Initialize the log\n",
    "custom_callback = CustomCallback(log)\n",
    "history = model.fit(\n",
    "    x={'input_ids': X_train[0], 'attention_mask': X_train[1]},\n",
    "    y=Y_train,\n",
    "    epochs=20,\n",
    "    batch_size=4,\n",
    "    callbacks=[custom_callback, lr_scheduler]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict({'input_ids': X_train[0], 'attention_mask': X_train[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 1)\n"
     ]
    }
   ],
   "source": [
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PDF</th>\n",
       "      <th>Label</th>\n",
       "      <th>Conference</th>\n",
       "      <th>Rationale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R007.pdf</td>\n",
       "      <td>Advancements in 3D Food Modeling: A Review of ...</td>\n",
       "      <td>1</td>\n",
       "      <td>CVPR</td>\n",
       "      <td>The MetaFood Workshop Challenge focuses on int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R011.pdf</td>\n",
       "      <td>Addressing Popularity Bias with Popularity-Con...</td>\n",
       "      <td>1</td>\n",
       "      <td>KDD</td>\n",
       "      <td>Contemporary recommender systems face populari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R002.pdf</td>\n",
       "      <td>Synergistic Convergence of Photosynthetic Path...</td>\n",
       "      <td>0</td>\n",
       "      <td>Non Publishable</td>\n",
       "      <td>This experimental research employs whimsical m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R009.pdf</td>\n",
       "      <td>The Importance of Written Explanations in\\nAgg...</td>\n",
       "      <td>1</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>A study of the \"wisdom of the crowd\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R004.pdf</td>\n",
       "      <td>AI-Driven Personalization in Online Education\\...</td>\n",
       "      <td>0</td>\n",
       "      <td>Non Publishable</td>\n",
       "      <td>We propose an unconventional method for incorp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  ...                                          Rationale\n",
       "0  R007.pdf  ...  The MetaFood Workshop Challenge focuses on int...\n",
       "1  R011.pdf  ...  Contemporary recommender systems face populari...\n",
       "2  R002.pdf  ...  This experimental research employs whimsical m...\n",
       "3  R009.pdf  ...               A study of the \"wisdom of the crowd\"\n",
       "4  R004.pdf  ...  We propose an unconventional method for incorp...\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "csv_file_path = r\"C:\\Users\\Debajyoti\\OneDrive\\Desktop\\project task-1\\data\\updated_publishable_data.csv\" \n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_label'] = prediction.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PDF</th>\n",
       "      <th>Label</th>\n",
       "      <th>Conference</th>\n",
       "      <th>Rationale</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R007.pdf</td>\n",
       "      <td>Advancements in 3D Food Modeling: A Review of ...</td>\n",
       "      <td>1</td>\n",
       "      <td>CVPR</td>\n",
       "      <td>The MetaFood Workshop Challenge focuses on int...</td>\n",
       "      <td>[0.7942856550216675]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R011.pdf</td>\n",
       "      <td>Addressing Popularity Bias with Popularity-Con...</td>\n",
       "      <td>1</td>\n",
       "      <td>KDD</td>\n",
       "      <td>Contemporary recommender systems face populari...</td>\n",
       "      <td>[0.7322335243225098]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R002.pdf</td>\n",
       "      <td>Synergistic Convergence of Photosynthetic Path...</td>\n",
       "      <td>0</td>\n",
       "      <td>Non Publishable</td>\n",
       "      <td>This experimental research employs whimsical m...</td>\n",
       "      <td>[0.484770804643631]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R009.pdf</td>\n",
       "      <td>The Importance of Written Explanations in\\nAgg...</td>\n",
       "      <td>1</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>A study of the \"wisdom of the crowd\"</td>\n",
       "      <td>[0.8507500886917114]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R004.pdf</td>\n",
       "      <td>AI-Driven Personalization in Online Education\\...</td>\n",
       "      <td>0</td>\n",
       "      <td>Non Publishable</td>\n",
       "      <td>We propose an unconventional method for incorp...</td>\n",
       "      <td>[0.6077845692634583]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>R010.pdf</td>\n",
       "      <td>Detecting Medication Usage in Parkinson’s Dise...</td>\n",
       "      <td>1</td>\n",
       "      <td>KDD</td>\n",
       "      <td>A transformer-based method for indoor localiza...</td>\n",
       "      <td>[0.8016510605812073]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>R015.pdf</td>\n",
       "      <td>Examining the Convergence of Denoising Diffusi...</td>\n",
       "      <td>1</td>\n",
       "      <td>TMLR</td>\n",
       "      <td>The results of a deep generative model are bas...</td>\n",
       "      <td>[0.8603323698043823]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>R003.pdf</td>\n",
       "      <td>Deciphering the Enigmatic Properties of Metals...</td>\n",
       "      <td>0</td>\n",
       "      <td>Non Publishable</td>\n",
       "      <td>The ephemeral and the mundane, as the luminesc...</td>\n",
       "      <td>[0.07095776498317719]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>R005.pdf</td>\n",
       "      <td>Analyzing Real-Time Group Coordination in\\nAug...</td>\n",
       "      <td>0</td>\n",
       "      <td>Non Publishable</td>\n",
       "      <td>AR enhances synchronization and fostering gest...</td>\n",
       "      <td>[0.537226676940918]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>R014.pdf</td>\n",
       "      <td>Addressing Min-Max Challenges in Nonconvex-Non...</td>\n",
       "      <td>1</td>\n",
       "      <td>TMLR</td>\n",
       "      <td>We propose a new convergence method for saddle...</td>\n",
       "      <td>[0.8397592306137085]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>R001.pdf</td>\n",
       "      <td>Transdimensional Properties of Graphite in Rel...</td>\n",
       "      <td>0</td>\n",
       "      <td>Non Publishable</td>\n",
       "      <td>The eerie case of the missing socks in the lau...</td>\n",
       "      <td>[0.12455511838197708]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>R012.pdf</td>\n",
       "      <td>Safe Predictors for Input-Output Specification...</td>\n",
       "      <td>1</td>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>We demonstrate the applicability of this metho...</td>\n",
       "      <td>[0.7858803272247314]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>R013.pdf</td>\n",
       "      <td>Generalization in ReLU Networks via Restricted...</td>\n",
       "      <td>1</td>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>We propose a method for evaluating the empiric...</td>\n",
       "      <td>[0.8757592439651489]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>R006.pdf</td>\n",
       "      <td>Detailed Action Identification in Baseball Gam...</td>\n",
       "      <td>1</td>\n",
       "      <td>CVPR</td>\n",
       "      <td>A review of the effectiveness of different mod...</td>\n",
       "      <td>[0.6886328458786011]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>R008.pdf</td>\n",
       "      <td>Advanced techniques for through and contextual...</td>\n",
       "      <td>1</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>Dual annotations improve the accuracy of a neu...</td>\n",
       "      <td>[0.8097798824310303]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  ...        predicted_label\n",
       "0   R007.pdf  ...   [0.7942856550216675]\n",
       "1   R011.pdf  ...   [0.7322335243225098]\n",
       "2   R002.pdf  ...    [0.484770804643631]\n",
       "3   R009.pdf  ...   [0.8507500886917114]\n",
       "4   R004.pdf  ...   [0.6077845692634583]\n",
       "5   R010.pdf  ...   [0.8016510605812073]\n",
       "6   R015.pdf  ...   [0.8603323698043823]\n",
       "7   R003.pdf  ...  [0.07095776498317719]\n",
       "8   R005.pdf  ...    [0.537226676940918]\n",
       "9   R014.pdf  ...   [0.8397592306137085]\n",
       "10  R001.pdf  ...  [0.12455511838197708]\n",
       "11  R012.pdf  ...   [0.7858803272247314]\n",
       "12  R013.pdf  ...   [0.8757592439651489]\n",
       "13  R006.pdf  ...   [0.6886328458786011]\n",
       "14  R008.pdf  ...   [0.8097798824310303]\n",
       "\n",
       "[15 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PDF</th>\n",
       "      <th>Label</th>\n",
       "      <th>Conference</th>\n",
       "      <th>Rationale</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R007.pdf</td>\n",
       "      <td>Advancements in 3D Food Modeling: A Review of ...</td>\n",
       "      <td>1</td>\n",
       "      <td>CVPR</td>\n",
       "      <td>The MetaFood Workshop Challenge focuses on int...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R011.pdf</td>\n",
       "      <td>Addressing Popularity Bias with Popularity-Con...</td>\n",
       "      <td>1</td>\n",
       "      <td>KDD</td>\n",
       "      <td>Contemporary recommender systems face populari...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R002.pdf</td>\n",
       "      <td>Synergistic Convergence of Photosynthetic Path...</td>\n",
       "      <td>0</td>\n",
       "      <td>Non Publishable</td>\n",
       "      <td>This experimental research employs whimsical m...</td>\n",
       "      <td>0</td>\n",
       "      <td>Non-publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R009.pdf</td>\n",
       "      <td>The Importance of Written Explanations in\\nAgg...</td>\n",
       "      <td>1</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>A study of the \"wisdom of the crowd\"</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R004.pdf</td>\n",
       "      <td>AI-Driven Personalization in Online Education\\...</td>\n",
       "      <td>0</td>\n",
       "      <td>Non Publishable</td>\n",
       "      <td>We propose an unconventional method for incorp...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>R010.pdf</td>\n",
       "      <td>Detecting Medication Usage in Parkinson’s Dise...</td>\n",
       "      <td>1</td>\n",
       "      <td>KDD</td>\n",
       "      <td>A transformer-based method for indoor localiza...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>R015.pdf</td>\n",
       "      <td>Examining the Convergence of Denoising Diffusi...</td>\n",
       "      <td>1</td>\n",
       "      <td>TMLR</td>\n",
       "      <td>The results of a deep generative model are bas...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>R003.pdf</td>\n",
       "      <td>Deciphering the Enigmatic Properties of Metals...</td>\n",
       "      <td>0</td>\n",
       "      <td>Non Publishable</td>\n",
       "      <td>The ephemeral and the mundane, as the luminesc...</td>\n",
       "      <td>0</td>\n",
       "      <td>Non-publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>R005.pdf</td>\n",
       "      <td>Analyzing Real-Time Group Coordination in\\nAug...</td>\n",
       "      <td>0</td>\n",
       "      <td>Non Publishable</td>\n",
       "      <td>AR enhances synchronization and fostering gest...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>R014.pdf</td>\n",
       "      <td>Addressing Min-Max Challenges in Nonconvex-Non...</td>\n",
       "      <td>1</td>\n",
       "      <td>TMLR</td>\n",
       "      <td>We propose a new convergence method for saddle...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>R001.pdf</td>\n",
       "      <td>Transdimensional Properties of Graphite in Rel...</td>\n",
       "      <td>0</td>\n",
       "      <td>Non Publishable</td>\n",
       "      <td>The eerie case of the missing socks in the lau...</td>\n",
       "      <td>0</td>\n",
       "      <td>Non-publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>R012.pdf</td>\n",
       "      <td>Safe Predictors for Input-Output Specification...</td>\n",
       "      <td>1</td>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>We demonstrate the applicability of this metho...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>R013.pdf</td>\n",
       "      <td>Generalization in ReLU Networks via Restricted...</td>\n",
       "      <td>1</td>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>We propose a method for evaluating the empiric...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>R006.pdf</td>\n",
       "      <td>Detailed Action Identification in Baseball Gam...</td>\n",
       "      <td>1</td>\n",
       "      <td>CVPR</td>\n",
       "      <td>A review of the effectiveness of different mod...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>R008.pdf</td>\n",
       "      <td>Advanced techniques for through and contextual...</td>\n",
       "      <td>1</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>Dual annotations improve the accuracy of a neu...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  ...           status\n",
       "0   R007.pdf  ...      Publishable\n",
       "1   R011.pdf  ...      Publishable\n",
       "2   R002.pdf  ...  Non-publishable\n",
       "3   R009.pdf  ...      Publishable\n",
       "4   R004.pdf  ...      Publishable\n",
       "5   R010.pdf  ...      Publishable\n",
       "6   R015.pdf  ...      Publishable\n",
       "7   R003.pdf  ...  Non-publishable\n",
       "8   R005.pdf  ...      Publishable\n",
       "9   R014.pdf  ...      Publishable\n",
       "10  R001.pdf  ...  Non-publishable\n",
       "11  R012.pdf  ...      Publishable\n",
       "12  R013.pdf  ...      Publishable\n",
       "13  R006.pdf  ...      Publishable\n",
       "14  R008.pdf  ...      Publishable\n",
       "\n",
       "[15 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['predicted_label'] = df['predicted_label'].apply(lambda x:x[0])\n",
    "df['status'] = df['predicted_label'].apply(lambda x: 'Publishable' if x > 0.5 else 'Non-publishable')\n",
    "\n",
    "df['predicted_label'] = df['predicted_label'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Csv file successfully saved at --->C:\\Users\\Debajyoti\\OneDrive\\Desktop\\project task-1\\data\\updated_csv_file_with_classification.csv\n"
     ]
    }
   ],
   "source": [
    "updated_csv_file = r\"C:\\Users\\Debajyoti\\OneDrive\\Desktop\\project task-1\\data\\updated_csv_file_with_classification.csv\"\n",
    "df.to_csv(updated_csv_file,index=False)\n",
    "print(f\"Csv file successfully saved at --->{updated_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path = r\"C:\\Users\\Debajyoti\\OneDrive\\Desktop\\project task-1\\data\\Papers\\Papers\"\n",
    "\n",
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    with open(pdf_file, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page in range(len(reader.pages)):\n",
    "            text += reader.pages[page].extract_text()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text perfectly saved at texts and names also\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "texts = []\n",
    "pdf_names = []\n",
    "\n",
    "for root,dirs,files in os.walk(test_file_path):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root,file)\n",
    "        pdf_names.append(file)\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        texts.append(text)\n",
    "        \n",
    "print(f\"Text perfectly saved at texts and names also\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PDF</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P001.pdf</td>\n",
       "      <td>Leveraging Clustering Techniques for Enhanced\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P002.pdf</td>\n",
       "      <td>Virus Propagation and their Far-Reaching\\nImpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P003.pdf</td>\n",
       "      <td>Explainable Reinforcement Learning for Financi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P004.pdf</td>\n",
       "      <td>Graph Neural Networks Without Training: Harnes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P005.pdf</td>\n",
       "      <td>Collaborative Clothing Segmentation and\\nIdent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>P131.pdf</td>\n",
       "      <td>Enhancing Disentanglement through Learned\\nAgg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>P132.pdf</td>\n",
       "      <td>Analyzing Fermentation Patterns with Multi-Mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>P133.pdf</td>\n",
       "      <td>Discontinuous Constituent Parsing as Sequence\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>P134.pdf</td>\n",
       "      <td>Unraveling the Enigmatic Parallels Between DNA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>P135.pdf</td>\n",
       "      <td>A Decentralized Local Stochastic Extragradient...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          PDF                                               Text\n",
       "0    P001.pdf  Leveraging Clustering Techniques for Enhanced\\...\n",
       "1    P002.pdf  Virus Propagation and their Far-Reaching\\nImpl...\n",
       "2    P003.pdf  Explainable Reinforcement Learning for Financi...\n",
       "3    P004.pdf  Graph Neural Networks Without Training: Harnes...\n",
       "4    P005.pdf  Collaborative Clothing Segmentation and\\nIdent...\n",
       "..        ...                                                ...\n",
       "130  P131.pdf  Enhancing Disentanglement through Learned\\nAgg...\n",
       "131  P132.pdf  Analyzing Fermentation Patterns with Multi-Mod...\n",
       "132  P133.pdf  Discontinuous Constituent Parsing as Sequence\\...\n",
       "133  P134.pdf  Unraveling the Enigmatic Parallels Between DNA...\n",
       "134  P135.pdf  A Decentralized Local Stochastic Extragradient...\n",
       "\n",
       "[135 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.DataFrame({'PDF':pdf_names,\n",
    "                       'Text':texts})\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = new_df['Text'].tolist()\n",
    "tokenized_test_data = tokenize_texts(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_input_ids, test_attention_masks = tokenized_test_data['input_ids'], tokenized_test_data['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 5s/step\n"
     ]
    }
   ],
   "source": [
    "X_test = (test_input_ids, test_attention_masks)\n",
    "prediction_test = model.predict({'input_ids': X_test[0], 'attention_mask': X_test[1]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PDF</th>\n",
       "      <th>Text</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P001.pdf</td>\n",
       "      <td>Leveraging Clustering Techniques for Enhanced\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P002.pdf</td>\n",
       "      <td>Virus Propagation and their Far-Reaching\\nImpl...</td>\n",
       "      <td>0</td>\n",
       "      <td>Non-Publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P003.pdf</td>\n",
       "      <td>Explainable Reinforcement Learning for Financi...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P004.pdf</td>\n",
       "      <td>Graph Neural Networks Without Training: Harnes...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P005.pdf</td>\n",
       "      <td>Collaborative Clothing Segmentation and\\nIdent...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>P131.pdf</td>\n",
       "      <td>Enhancing Disentanglement through Learned\\nAgg...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>P132.pdf</td>\n",
       "      <td>Analyzing Fermentation Patterns with Multi-Mod...</td>\n",
       "      <td>0</td>\n",
       "      <td>Non-Publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>P133.pdf</td>\n",
       "      <td>Discontinuous Constituent Parsing as Sequence\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>P134.pdf</td>\n",
       "      <td>Unraveling the Enigmatic Parallels Between DNA...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>P135.pdf</td>\n",
       "      <td>A Decentralized Local Stochastic Extragradient...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          PDF  ...           status\n",
       "0    P001.pdf  ...      Publishable\n",
       "1    P002.pdf  ...  Non-Publishable\n",
       "2    P003.pdf  ...      Publishable\n",
       "3    P004.pdf  ...      Publishable\n",
       "4    P005.pdf  ...      Publishable\n",
       "..        ...  ...              ...\n",
       "130  P131.pdf  ...      Publishable\n",
       "131  P132.pdf  ...  Non-Publishable\n",
       "132  P133.pdf  ...      Publishable\n",
       "133  P134.pdf  ...      Publishable\n",
       "134  P135.pdf  ...      Publishable\n",
       "\n",
       "[135 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['predicted_label'] = prediction_test.tolist()\n",
    "new_df['predicted_label'] = new_df['predicted_label'].apply(lambda x:x[0])\n",
    "new_df['status'] = new_df['predicted_label'].apply(lambda x: 'Publishable' if x>=0.5 else 'Non-Publishable')\n",
    "new_df['predicted_label'] = new_df['predicted_label'].apply(lambda x: 1 if x>=0.5 else 0)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total no. of non publishable paper: 47\n"
     ]
    }
   ],
   "source": [
    "no_of_nonPublishable_paper = (new_df['predicted_label'] == 0).sum()\n",
    "print(\"total no. of non publishable paper:\",no_of_nonPublishable_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test csv file successfully transforted to C:\\Users\\Debajyoti\\OneDrive\\Desktop\\project task-1\\data\\test_dataframe.csv\n"
     ]
    }
   ],
   "source": [
    "test_csv_file = r\"C:\\Users\\Debajyoti\\OneDrive\\Desktop\\project task-1\\data\\test_dataframe.csv\"\n",
    "new_df.to_csv(test_csv_file)\n",
    "print(f\"Test csv file successfully transforted to {test_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>PDF</th>\n",
       "      <th>Text</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>status</th>\n",
       "      <th>Rationale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>P001.pdf</td>\n",
       "      <td>Leveraging Clustering Techniques for Enhanced\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "      <td>A clustering-based learning detection strategy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>P002.pdf</td>\n",
       "      <td>Virus Propagation and their Far-Reaching\\nImpl...</td>\n",
       "      <td>0</td>\n",
       "      <td>Non-Publishable</td>\n",
       "      <td>The cellular mechanisms underlying viral repli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>P003.pdf</td>\n",
       "      <td>Explainable Reinforcement Learning for Financi...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "      <td>A new approach to financial market simulation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>P004.pdf</td>\n",
       "      <td>Graph Neural Networks Without Training: Harnes...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "      <td>GNNs are a reliable yet relatively unexplored ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>P005.pdf</td>\n",
       "      <td>Collaborative Clothing Segmentation and\\nIdent...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "      <td>The system is a novel image co-segmentation sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>130</td>\n",
       "      <td>P131.pdf</td>\n",
       "      <td>Enhancing Disentanglement through Learned\\nAgg...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "      <td>We use the implicit inductive bias in ImageNet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>131</td>\n",
       "      <td>P132.pdf</td>\n",
       "      <td>Analyzing Fermentation Patterns with Multi-Mod...</td>\n",
       "      <td>0</td>\n",
       "      <td>Non-Publishable</td>\n",
       "      <td>A new generative model of sourdough bread has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>132</td>\n",
       "      <td>P133.pdf</td>\n",
       "      <td>Discontinuous Constituent Parsing as Sequence\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "      <td>Discontinuous constituent parsing is a method ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>133</td>\n",
       "      <td>P134.pdf</td>\n",
       "      <td>Unraveling the Enigmatic Parallels Between DNA...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "      <td>The Flumplenook hypothesis is a theoretical fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>134</td>\n",
       "      <td>P135.pdf</td>\n",
       "      <td>A Decentralized Local Stochastic Extragradient...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "      <td>We propose a stochastic extragradient method f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  ...                                          Rationale\n",
       "0             0  ...  A clustering-based learning detection strategy...\n",
       "1             1  ...  The cellular mechanisms underlying viral repli...\n",
       "2             2  ...  A new approach to financial market simulation ...\n",
       "3             3  ...  GNNs are a reliable yet relatively unexplored ...\n",
       "4             4  ...  The system is a novel image co-segmentation sy...\n",
       "..          ...  ...                                                ...\n",
       "130         130  ...  We use the implicit inductive bias in ImageNet...\n",
       "131         131  ...  A new generative model of sourdough bread has ...\n",
       "132         132  ...  Discontinuous constituent parsing is a method ...\n",
       "133         133  ...  The Flumplenook hypothesis is a theoretical fr...\n",
       "134         134  ...  We propose a stochastic extragradient method f...\n",
       "\n",
       "[135 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_with_rationale_csv = r\"C:\\Users\\Debajyoti\\OneDrive\\Desktop\\project task-1\\data\\test_with_rationale.csv\"\n",
    "test_df = pd.read_csv(test_with_rationale_csv)\n",
    "test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 5s/step\n"
     ]
    }
   ],
   "source": [
    "from categorical_model_load import categorical_model\n",
    "prediction_categorical_test = categorical_model.predict({'input_ids': X_test[0], 'attention_mask': X_test[1]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135, 6)\n"
     ]
    }
   ],
   "source": [
    "print(prediction_categorical_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135,)\n"
     ]
    }
   ],
   "source": [
    "final_prediction = tf.argmax(prediction_categorical_test,axis=1)\n",
    "print(final_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    0: 'Null',       \n",
    "    1: 'TMLR',\n",
    "    2: 'EMNLP',\n",
    "    3: 'NeurIPS',\n",
    "    4: 'KDD',\n",
    "    5: 'CVPR'\n",
    "}\n",
    "final_labels = [mapping[prediction.numpy()] for prediction in final_prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>PDF</th>\n",
       "      <th>Text</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>status</th>\n",
       "      <th>Rationale</th>\n",
       "      <th>Conference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>P001.pdf</td>\n",
       "      <td>Leveraging Clustering Techniques for Enhanced\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "      <td>A clustering-based learning detection strategy...</td>\n",
       "      <td>CVPR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>P002.pdf</td>\n",
       "      <td>Virus Propagation and their Far-Reaching\\nImpl...</td>\n",
       "      <td>0</td>\n",
       "      <td>Non-Publishable</td>\n",
       "      <td>The cellular mechanisms underlying viral repli...</td>\n",
       "      <td>Null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>P003.pdf</td>\n",
       "      <td>Explainable Reinforcement Learning for Financi...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "      <td>A new approach to financial market simulation ...</td>\n",
       "      <td>Null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>P004.pdf</td>\n",
       "      <td>Graph Neural Networks Without Training: Harnes...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "      <td>GNNs are a reliable yet relatively unexplored ...</td>\n",
       "      <td>NeurIPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>P005.pdf</td>\n",
       "      <td>Collaborative Clothing Segmentation and\\nIdent...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "      <td>The system is a novel image co-segmentation sy...</td>\n",
       "      <td>CVPR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>130</td>\n",
       "      <td>P131.pdf</td>\n",
       "      <td>Enhancing Disentanglement through Learned\\nAgg...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "      <td>We use the implicit inductive bias in ImageNet...</td>\n",
       "      <td>CVPR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>131</td>\n",
       "      <td>P132.pdf</td>\n",
       "      <td>Analyzing Fermentation Patterns with Multi-Mod...</td>\n",
       "      <td>0</td>\n",
       "      <td>Non-Publishable</td>\n",
       "      <td>A new generative model of sourdough bread has ...</td>\n",
       "      <td>Null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>132</td>\n",
       "      <td>P133.pdf</td>\n",
       "      <td>Discontinuous Constituent Parsing as Sequence\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "      <td>Discontinuous constituent parsing is a method ...</td>\n",
       "      <td>NeurIPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>133</td>\n",
       "      <td>P134.pdf</td>\n",
       "      <td>Unraveling the Enigmatic Parallels Between DNA...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "      <td>The Flumplenook hypothesis is a theoretical fr...</td>\n",
       "      <td>CVPR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>134</td>\n",
       "      <td>P135.pdf</td>\n",
       "      <td>A Decentralized Local Stochastic Extragradient...</td>\n",
       "      <td>1</td>\n",
       "      <td>Publishable</td>\n",
       "      <td>We propose a stochastic extragradient method f...</td>\n",
       "      <td>EMNLP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  ... Conference\n",
       "0             0  ...       CVPR\n",
       "1             1  ...       Null\n",
       "2             2  ...       Null\n",
       "3             3  ...    NeurIPS\n",
       "4             4  ...       CVPR\n",
       "..          ...  ...        ...\n",
       "130         130  ...       CVPR\n",
       "131         131  ...       Null\n",
       "132         132  ...    NeurIPS\n",
       "133         133  ...       CVPR\n",
       "134         134  ...      EMNLP\n",
       "\n",
       "[135 rows x 7 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['Conference'] = final_labels\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_csv_file = r\"C:\\Users\\Debajyoti\\OneDrive\\Desktop\\project task-1\\data\\final_result.csv\"\n",
    "test_df.to_csv(final_csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
